{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azure.quantum import Workspace\n",
        "workspace = Workspace (\n",
        "   resource_id = \"/subscriptions/94f5e81d-8575-4e4a-aa46-55606a36edac/resourceGroups/AzureQuantum/providers/Microsoft.Quantum/Workspaces/Machine-Learning-jUPYTER-Notebook\",\n",
        "   location = \"eastus\"\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing (NLP)\n",
        " This is the computing that deals with communication between humans and computers examples are speelcheck or autocomplete.\n",
        "# Recurrent  Neural Networks.\n",
        "The neural network that we will use will be much capabal of prosessing seuential data such as text or characters called a recurrent Neural Network (RNN for short)\n",
        "we will learnhow to use a RNN to do the following\n",
        "- sentiment Analysis( this is basically finding out how positive or negative a peice of information is)\n",
        "- Character Generation\n",
        "# \n",
        "RNNs come in very complex form so we will learn how they work  and the problems they are suited for."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence Data\n",
        "This is unlike image data that has width height and channels, text data that we are going to use will be special. in that  the textual data contains many words that follow ina very specific and meaningful order we need to keep track of each word and when it occurs in the data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Text\n",
        "As we know  machine learning models and neural networks dont take raw data as input.\n",
        "Howe we change text into numerical data methods:\n",
        "#\n",
        "- Bag of words (only works on simple tasks )"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example of code for the Bag of words how it works"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {} # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "    global word_encoding\n",
        "    words = text.lower().split(\" \")# create a lits of all of the words in the text\n",
        "    bag={}\n",
        "    for word in words:\n",
        "        if word in vocab:\n",
        "            encoding = vocab[word] # get encoding from vocab\n",
        "        else:\n",
        "            vocab[word] = word_encoding\n",
        "            encoding= word_encoding\n",
        "            word_encoding += 1\n",
        "        \n",
        "        if encoding in bag:\n",
        "            bag[encoding] += 1\n",
        "        else:\n",
        "            bag[encoding] =1\n",
        "    \n",
        "    return bag\n",
        "text = \"This is a test to see if this test will work is is test a a\" \n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORD EMBEDING\n",
        "This is a vectorized representation of words in a given documment that places words with similar meanings near each other."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks (RNN's)\n",
        "This has an internal loop in it, in the sense that it does not procsess the data all at once.\n",
        "# \n",
        "It has a recurrent layer loop"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM (Long Short-Term Memory)\n",
        "this layer works like a LNN but it adds another way to access inputs from any timestep in the past.\n",
        "# \n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis (this is to determine if a block of words is positive ore negative)\n",
        "We are going to be using dataset of movie review"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The movie review dataset contains 25000 reveiws from IMDB where each one is already processed and has a label as either positive or negative.  each reveiw is encoded by intergers that represents how common a word is in the entire dataset# For example a word encoded by the integer 3 means that it is the 3rd most common word in the dataset."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Always make sure tensorflow is installed if not use this code\n",
        "!pip install tensorflow  # if your in a notebook"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np \n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN =250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-08-13 18:28:42.442037: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-08-13 18:28:42.517062: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-08-13 18:28:42.521871: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-08-13 18:28:44.772220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17464789/17464789 [==============================] - 0s 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets look at one review\n",
        "train_data[0]\n",
        "len(train_data[0])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "218"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Preprocessing\n",
        "Remeber we cannot feed Neural Networks or machine learning models with differnt sizes of data. we have to make the data same size.\n",
        "# \n",
        "Example ower data is not of the same lengths. therefor we must make each review the same length to do this we will follow the procedure below:\n",
        "\n",
        "- If the review is greater than 250 words then trim off the extra words\n",
        "- If the review is less than 250 words add the necessary amount 0's to make is equal to 250 words\n",
        "\n",
        "Luckily for us keras has a function that can do this for us"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Model\n",
        "Now we will use a word embedding layer as first layer in our model and add a LSTM layers afterwards that feeds into a dense node to get our predicted sentiment.\n",
        "# \n",
        "32 stands for the output dimensions of the vectores generated by the embedding layers. we can change this value if we want to."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),              # sigmoid squishes the values to be between 0 and 1\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid') # here we are trying to predict if the setiment is true or false we use the...\n",
        "                                                    # Dense 0 - 1 so that if the answer is greater than 0.5 then we can rate that...\n",
        "                                                    # a positive review and vise verser \n",
        "])\n",
        "#the model summary\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_7 (Embedding)     (None, None, 32)          2834688   \n                                                                 \n lstm_7 (LSTM)               (None, 32)                8320      \n                                                                 \n dense_7 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 2843041 (10.85 MB)\nTrainable params: 2843041 (10.85 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "Now its time to compile and train our model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['acc']) # this is compiling the model\n",
        "history = model.fit(train_data, train_labels, epochs = 10, validation_split = 0.2) #( here we will use only 0.2 (20%)of the data tpo evaluate the model)\n",
        "# binary_crossentropy this tells us how far we are from the correct probability coz we are predicting either zero or one\n",
        "# getting a high accureacy basically tells us that we are either using not enough data, or change the model."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/10\n625/625 [==============================] - 98s 153ms/step - loss: 0.4482 - acc: 0.7818 - val_loss: 0.2932 - val_acc: 0.8784\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/10\n625/625 [==============================] - 88s 141ms/step - loss: 0.2617 - acc: 0.9022 - val_loss: 0.3067 - val_acc: 0.8782\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/10\n625/625 [==============================] - 84s 135ms/step - loss: 0.2006 - acc: 0.9262 - val_loss: 0.2878 - val_acc: 0.8840\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/10\n625/625 [==============================] - 84s 134ms/step - loss: 0.1645 - acc: 0.9435 - val_loss: 0.3552 - val_acc: 0.8750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/10\n625/625 [==============================] - 87s 139ms/step - loss: 0.1360 - acc: 0.9528 - val_loss: 0.3352 - val_acc: 0.8866\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/10\n625/625 [==============================] - 84s 134ms/step - loss: 0.1139 - acc: 0.9616 - val_loss: 0.4590 - val_acc: 0.8612\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 7/10\n625/625 [==============================] - 89s 142ms/step - loss: 0.0964 - acc: 0.9693 - val_loss: 0.3506 - val_acc: 0.8632\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 8/10\n625/625 [==============================] - 88s 140ms/step - loss: 0.0816 - acc: 0.9750 - val_loss: 0.3976 - val_acc: 0.8540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 9/10\n625/625 [==============================] - 91s 146ms/step - loss: 0.0668 - acc: 0.9798 - val_loss: 0.4676 - val_acc: 0.8662\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 10/10\n625/625 [==============================] - 84s 134ms/step - loss: 0.0601 - acc: 0.9819 - val_loss: 0.4062 - val_acc: 0.8802\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### And well evaluate the model on our training data  to see how weel it performs"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "782/782 [==============================] - 37s 47ms/step - loss: 0.4760 - acc: 0.8619\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n[0.4759834408760071, 0.8619199991226196]\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### So we are scorring somewhere in the mid-high 80s not bad for a smple recurrent network"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Predictions\n",
        "Now by using our network lets make predictions on our own reviews\n",
        "# \n",
        "Since our review are encoded we will need to convert any review that we right into that form so the network can understand it. \n",
        "# \n",
        "To do that well load the encodings from the dataset and use them to encode our own data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index =  imdb.get_word_index()\n",
        "def encode_text(text):\n",
        "    tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
        "    tokens = [word_index[word] if word  in word_index else 0 for word in tokens]\n",
        "    return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# While we are at it lets make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text= \"\"\n",
        "    for num in integers:\n",
        "        if num != PAD:\n",
        "            text += reverse_word_index[num] +  \" \"\n",
        "    return text[:-1]\n",
        "print(decode_integers(encoded))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "that movie was just amazing so amazing\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NOW time to make a prediction\n",
        "def predict(text):\n",
        "    encoded_text = encode_text(text)\n",
        "    pred = np.zeros((1,250)) # this is becouse our model expects numbers from 1 to 250\n",
        "    pred[0] = encoded_text\n",
        "    result = model.predict(pred)\n",
        "    print(result[0])\n",
        "\n",
        "positive_review = \"That movie was so awesome! I really Love it and would watch it again becouse it was amazingly great\"\n",
        "predict(positive_review)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\r1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 78ms/step\n[0.53327113]\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_review = \" that movie sucked. I hated it and wouldn't watch it again. was one of the worst things i have ever watched\"\n",
        "predict(negative_review)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\r1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 65ms/step\n[0.8039897]\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Go to next notebook 1.1 to create a model with RNN that generates / predicts next characters "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.9.17",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}