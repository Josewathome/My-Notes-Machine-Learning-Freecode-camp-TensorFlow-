{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning (R.L)\n",
        "this type of machine learning is differnt from all that we have learnt\n",
        "\n",
        "This type of Learning is used in training agents (an AI) to interact with enviroments like games.\n",
        "\n",
        "Rather than feeding our machine learning model millions of examples we let the model come up with its own examples by exploring an enviroment, this is same as humans learning from our mistakes and past experiences.\n"
      ],
      "metadata": {
        "id": "VJ4Uy6r1Miw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Terminology\n",
        "\n",
        "\n",
        "*   Enviroment - In R.L tasks we have the notion enviroment. this is what our agent will explore. an example of an enviroment in the case of training an AI to play say a game of mario would be the level we are training the agent on.\n",
        "\n",
        "*   AGENT - this is an entity that is exproring the enviroment. our agent will intaract and take different actions within the enviroment. in our mario example the character within the game will be our agent.\n",
        "\n",
        "\n",
        "*   Action - any interaction between the agent and the enviroment would be considerd an action. an action may or may not change the current state of the agent. infact the act of doing nothing si actually an action as well. the action of say not pressing a kay if we are usning mario example\n",
        "\n",
        "*   Reward - every action that our agent takes will result in a reward of some magnitude(positive or negative) the Goal of our agent is to maximize its reward in an enviroment, sometimes the reward might be clear, example is if an agent performs an action which increases their score in the inviroment we could say they've received a positive reward. and vise verse\n",
        "\n",
        "the most important part of Reinforcemnt learning is to determine how to reward the agent, after all the goal of the agent is to maximize its rewards this means we should reward the agent appropiatly such as it reaches the desired goal.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DGvy7HoYOZN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qw-EW0xuX0OL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q- LEARNING\n",
        "this is simple yet poerful techneque in machine learning that involves learning a matrix of action-reward values. this matrix is often refferd to as Q table or Q - Matrix\n"
      ],
      "metadata": {
        "id": "g-U2zZPfX4FS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learnig the Q- Table\n",
        "I will start by saying our table starts with all 0 values because the agent has yet to learn anything about tthe enviroment\n",
        "\n",
        "our agents learns by exploring the enviroment and observing the outcome/reward from each action it takes in a given state. but how does it know what action to take in each state? there are two ways that our agent can decide to use in selecting action\n",
        "\n",
        "\n",
        "1.   Randomly picking a valid action\n",
        "2.   Using the current q-table to find the best action.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZF6gyLyZ8bW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q - Learning code\n",
        "we are going to train an agent to navigate a popular enviroment from Open AI Gym.it was created so as to practise machine learning using Unique enviroments\n",
        "\n",
        "lets start by looking what Open AI Gym is"
      ],
      "metadata": {
        "id": "WEOyXVRvejYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym # all you need to do is just open ai gym and use it"
      ],
      "metadata": {
        "id": "j7TycOCqfgIV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you import gym you can load an enviroment using the line\n",
        "\n",
        "gym.make(\"enviroment\")"
      ],
      "metadata": {
        "id": "6kKxfQLxf5EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake\") # we are going to use FrozenLake enviroment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztTIRGFhgJFz",
        "outputId": "097932b4-7c74-4d8d-b3da-815e271011f4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:601: UserWarning: \u001b[33mWARN: Using the latest versioned environment `FrozenLake-v1` instead of the unversioned environment `FrozenLake`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "there ae a few other commands that can be used to interact and get information about the enviroment"
      ],
      "metadata": {
        "id": "YxDkxHUhgsM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space.n) # get number of states\n",
        "print(env.action_space.n) # get number of actions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o-64UTWgbmJ",
        "outputId": "1d0f8694-21a2-4b61-fda4-ddf6cbd886b2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset() # Reset enviroments to default default state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUgxnUAQhuIs",
        "outputId": "d5baafe2-25ad-4f2d-873b-570ea3c89c07"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action = env.action_space.sample() # get a random action\n",
        "print(action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDPzJ0hEh-5z",
        "outputId": "6afa40c1-aae6-4e7e-c336-5baf9d65a2e0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_state, reward, done, info = env.step(action) # take action, notice it returns  information about the action"
      ],
      "metadata": {
        "id": "owOFGSBTiOL2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render() # render the GUI for the enviroment\n"
      ],
      "metadata": {
        "id": "va6RGNBoibG0"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frozen Lake Enviroment"
      ],
      "metadata": {
        "id": "0T6B25utk4VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is what we are going to do\n",
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "env = gym.make('FrozenLake-v1')\n",
        "STATES = env.observation_space.n\n",
        "ACTIONS =env.action_space.n"
      ],
      "metadata": {
        "id": "Yv81has2kZrK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q =np.zeros((STATES, ACTIONS)) # CREATE a matrix with all 0 values\n",
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beu9AUNMoqyB",
        "outputId": "46c65cca-10f7-421d-aeb6-bd0633639792"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants\n",
        "we need to define some constants to be used to update our Q- Table and tellour agent when to stop training"
      ],
      "metadata": {
        "id": "QYLEc9e2pDha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODES = 10000 # HOW MANY TIMES TO RUN THE ENVIROMENT FROM BEGINNING\n",
        "MAX_STEPS = 100 # Max numbers of steps alliwed for each run of enviroment\n",
        "\n",
        "LEARNING_RATE = 0.81 # the learning rate ( the higher it is the faster it will learn)\n",
        "GAMMA = 0.96"
      ],
      "metadata": {
        "id": "DzCYLoG6o80d"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Picking an Action\n",
        "we can pick an action using the one of the two mwthods\n",
        "\n",
        "1.   Randomly Picking a valid action\n",
        "2.   using the current Q - Table to find the best action\n",
        "\n",
        "Here we will define a new value E that will tell us the probability of selecting a random action. this action will start very High and slowly decrease as the agent learns more about the enviroment"
      ],
      "metadata": {
        "id": "xdGpoIJxqM2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.9 # start with a 90% chance of picking a random action\n",
        "\n",
        "# code to pick action\n",
        "if np.random.uniform(0, 1)< epsilon: # We will check if randomly selected value is less than epsilon\n",
        "    action = env.action_space.sample() # take random action\n",
        "\n",
        "else:\n",
        "  action = np.argmax(Q[state, :]) # use   Q table to pick best action based on current values"
      ],
      "metadata": {
        "id": "WobOyUITq7CD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updating Q values\n",
        "the code below impliments the fomular"
      ],
      "metadata": {
        "id": "pThtq6EeswlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q[state,action] = Q[state,action] +LEARNING_RATE * (reward + GAMMA * np.max(Q[new_state, :]) - Q[state,action] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "_AHkL21btlmB",
        "outputId": "933487b2-0562-4872-cca3-289c7fb57174"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-37a63cbc8777>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mGAMMA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
          ]
        }
      ]
    }
  ]
}