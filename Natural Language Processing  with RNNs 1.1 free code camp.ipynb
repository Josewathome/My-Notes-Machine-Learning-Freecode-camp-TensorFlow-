{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "outputs": [],
      "execution_count": 74,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "gUzdtc1HCU26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN PlaY Generator\n",
        "Now we are going to use RNN to generate a next character for us when typing , we want it to recreate and it will learn how to write a version of its own.\n",
        "#\n",
        " we will do this using a character predictive model that will take as input a variable length sequence and predict the next character for us\n",
        "#\n",
        "we can use the model many times in a row with the output from the last prediction as the input for the next till we get a play/ good paragraph\n",
        "#\n",
        "\n",
        "all this guides are in : https://www.tensorflow.org/tutorials/text/text_generation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "dN4heId8CU29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Always make sure tensorflow is installed if not use this code\n",
        "!pip install tensorflow  # if your in a notebook"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Mk1G0zDcCU3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "outputs": [],
      "execution_count": 75,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "pzsIMbYYCU3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Set\n",
        " For this example we only need one peice of data. infact we can wright our own poem and pass that to the network for training if we'd like. however lets yuse something esay  lets use from a shakespear play"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Ntc5kFkMCU3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\", \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
      ],
      "outputs": [],
      "execution_count": 76,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "roAfioWFCU3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading Your Own Data\n",
        "to load your own data you'll need to upload a file from the dialog below. the you'll need to follow the steps from above but load in this new file insead\n",
        "#\n",
        "uncomment it to use the code below and make sure the file you load is a .txt file from your machine"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "C2Y3wx-9CU3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#from google.colab import files\n",
        "#path_to_file =  list(files.upload().keys())[0]"
      ],
      "outputs": [],
      "execution_count": 77,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "41ds4IRoCU3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Contents of file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Hl37KId2CU3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read, then decode for py2 compat\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Length of text is the number of characters in it\n",
        "print (\" length of text: {} characters\".format(len(text)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " length of text: 1115394 characters\n"
          ]
        }
      ],
      "execution_count": 78,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq4yT12iCU3H",
        "outputId": "06b6700a-ff7c-4be1-9d94-80e40f0dde39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first 250 Characters in text\n",
        "print(text[:250])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "execution_count": 79,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZFTd0-OCU3H",
        "outputId": "5b2270f7-b93e-45d2-dc61-3d1d1ffc1da8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding\n",
        "Since this text isint encoded yet well need to do that ourself. we are going to encode each unique characters as differnt integer"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "JvGmQ3PFCU3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "#creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "    return np.array([char2idx[c] for c in text])\n",
        "text_as_int = text_to_int(text)\n"
      ],
      "outputs": [],
      "execution_count": 80,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "nLh4EmjmCU3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets look at how part of our text is encoded\n",
        "print('text: ', text[:13])\n",
        "print(\"Encoded: \",text_to_int(text[:13]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  First Citizen\n",
            "Encoded:  [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ],
      "execution_count": 81,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y14vjU1aCU3J",
        "outputId": "9d48ec91-70f4-49fb-9d17-5b995680376a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### And here we will make a function that can convert our numeric values to text"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "hwU5RCZaCU3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "    try:\n",
        "        ints = ints.numpy()\n",
        "    except:\n",
        "        pass\n",
        "    return ''. join(idx2char[ints])\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ],
      "execution_count": 82,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw2SST95CU3J",
        "outputId": "6cec905b-2125-49d5-d493-8c94f008d027"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Training Examples\n",
        "Remeber our task is to feed the model a sequence and have it return to us the next character. this means we need to split our text data from above into shorter sequences that we can pass to the model as training examples\n",
        "#\n",
        "the training example we will prepare will use a seq_length sequence as input  and seq_length sequence as the output where that sequence is the original sequence shifted one letter to the right example:\n",
        "#\n",
        "input: Hell | output: ello\n",
        "#\n",
        "our first task is to create a stream of characters from our text data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "mnvfXHYACU3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100 # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length + 1)\n",
        "\n",
        "#Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "outputs": [],
      "execution_count": 83,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "EmNi7jlGCU3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Next we can  use the batch method to turn this stream of characters into bactches of desired length"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "tQnMLU5iCU3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)"
      ],
      "outputs": [],
      "execution_count": 84,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "iuhKLFurCU3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Now we use this sequences of length 101 and split them into  input and output."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "uYyknS0BCU3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk): # FOR example: hello\n",
        "    input_text = chunk[:-1] # hell\n",
        "    target_text = chunk[1:] # ello\n",
        "    return input_text, target_text # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target) # we use map to apply the above function to every entry"
      ],
      "outputs": [],
      "execution_count": 85,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "xdmi65oHCU3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "    print(\"\\n\\nEXAMPLE\\n\")\n",
        "    print(\"INPUT\")\n",
        "    print(int_to_text(x))\n",
        "    print(\"\\nOUTPUT\")\n",
        "    print(int_to_text(y))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Befor\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before\n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            " we proceed any furt\n",
            "\n",
            "OUTPUT\n",
            "we proceed any furth\n"
          ]
        }
      ],
      "execution_count": 86,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekp16gtHCU3M",
        "outputId": "be3c613d-dd88-4030-d5d8-354b4a361edc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Finally we need to make training batches"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "FU1cIKIICU3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab) # vocab is number of unique Characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "#  so it doesent attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements)\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
        "print(data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 20), dtype=tf.int64, name=None), TensorSpec(shape=(64, 20), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "execution_count": 87,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECOjQD0kCU3N",
        "outputId": "a904b5c9-8426-4369-c7c9-1a53be64a19b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Model\n",
        "now we will use an embedding layer as LSTM and one dense layer that contains node for each unique Character in our training data the dense layer will give us a probability distribution over all nodes"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "_OtJ3F_uCU3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the reson we are first making a function is that later we are going to be pusing bactches of 64size data\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences= True,\n",
        "                            stateful = True,\n",
        "                            recurrent_initializer = 'glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    ])\n",
        "    return model\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM,RNN_UNITS,BATCH_SIZE)\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "execution_count": 88,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K35KHrxjCU3O",
        "outputId": "7f6c52e4-a463-4822-e156-722d082d412e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Loss Function\n",
        "We will create our own loss function , this is becouse our model will output a (64, sequence_lenth, 65) shaped tensor that represents the probability distribution of each characterat each timestep for every sequence in the batch.\n",
        "#\n",
        "be4 we do this lets first look at input and output of our untraiend data so wecan understand what the model is actually giving us"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "RlKEwX2BCU3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data\n",
        "    print(example_batch_predictions.shape, \" #(batch_size, sequence_length, vocab_size)\")# print out the output shape"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._iterations\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 20, 65)  #(batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "execution_count": 89,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St9zqOi9CU3P",
        "outputId": "76da257f-47fd-4445-d60f-6e7e928f7031"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the prediction is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-2.9305695e-05 -4.2956294e-03 -5.2133258e-03 ...  9.5791626e-04\n",
            "   -3.4533191e-04  3.2098237e-03]\n",
            "  [-1.3752125e-03 -5.8074058e-03 -5.7119508e-03 ...  2.7252191e-03\n",
            "   -1.4251822e-03  1.1254540e-02]\n",
            "  [ 5.8382819e-03 -6.6267042e-03 -3.8421443e-03 ...  1.3222675e-03\n",
            "    3.1737301e-03  8.9204768e-03]\n",
            "  ...\n",
            "  [ 8.2987072e-03  1.0339039e-02  1.2308577e-02 ...  4.0844036e-03\n",
            "    8.1793619e-03  2.5647180e-04]\n",
            "  [-1.1709656e-03  1.1991293e-02  4.1806553e-03 ... -2.0262958e-03\n",
            "    1.1298696e-02  2.6900638e-03]\n",
            "  [-3.6952903e-03  1.2396922e-02  1.1973148e-02 ...  1.3705932e-03\n",
            "    1.1277175e-02  1.5380629e-03]]\n",
            "\n",
            " [[-2.9305695e-05 -4.2956294e-03 -5.2133258e-03 ...  9.5791626e-04\n",
            "   -3.4533191e-04  3.2098237e-03]\n",
            "  [-5.8811123e-04 -5.6665693e-04 -4.3918639e-03 ...  2.6377849e-04\n",
            "   -6.3296678e-03  4.3960810e-03]\n",
            "  [ 1.1733968e-03 -3.7005169e-03 -2.3881635e-03 ...  7.7000959e-03\n",
            "   -8.7662712e-03 -8.3507347e-04]\n",
            "  ...\n",
            "  [-9.1525670e-03  1.3997662e-02  8.5285744e-03 ... -2.1275708e-03\n",
            "   -1.9795818e-03  3.7306570e-04]\n",
            "  [-2.8040092e-03  1.3201418e-02  7.7241738e-03 ... -3.1830079e-03\n",
            "    6.6636817e-04 -1.6658995e-03]\n",
            "  [-5.1951967e-03  1.9890580e-02  7.5408295e-03 ... -6.2129681e-04\n",
            "    5.6062415e-03  3.4045195e-05]]\n",
            "\n",
            " [[-1.5264137e-04  3.8667845e-03  1.4822456e-03 ...  8.2506065e-04\n",
            "    4.6021701e-04 -5.9370976e-03]\n",
            "  [-3.6165698e-03 -2.2868542e-03 -2.0063482e-03 ... -2.9486336e-03\n",
            "    2.0555360e-03  5.4588215e-04]\n",
            "  [-6.3447738e-03 -1.9983875e-03 -7.4140378e-04 ... -2.7486854e-03\n",
            "   -1.4676442e-03  6.5444941e-03]\n",
            "  ...\n",
            "  [ 2.1809782e-03 -5.8177575e-03 -1.9759606e-03 ... -3.9765853e-03\n",
            "   -1.5484188e-03  1.8619933e-03]\n",
            "  [ 2.0799118e-03  2.3345940e-04 -8.4375264e-05 ... -1.8742820e-03\n",
            "   -2.8346041e-03 -6.0881642e-03]\n",
            "  [ 4.5898859e-03 -5.2607465e-03  3.2570397e-03 ... -2.0834515e-03\n",
            "   -1.6275903e-03 -4.0539796e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.2430810e-03 -4.0265208e-04  7.4395246e-04 ... -6.0191791e-04\n",
            "   -2.3876352e-03  6.8200594e-03]\n",
            "  [ 3.0091999e-03 -3.3397370e-03  4.1943431e-04 ...  7.9062593e-05\n",
            "   -2.8256807e-03  4.4096611e-03]\n",
            "  [ 2.8938199e-03 -5.7506338e-03 -5.2114497e-03 ...  8.7324344e-04\n",
            "   -2.4797323e-03  6.3118627e-03]\n",
            "  ...\n",
            "  [-9.8936437e-03  1.2207640e-02  2.9630472e-03 ... -2.4040448e-03\n",
            "   -1.9330912e-03  7.1322634e-03]\n",
            "  [-2.4688062e-03  1.0568408e-02  3.5916776e-03 ... -2.4037834e-03\n",
            "   -2.4585484e-04  3.9039452e-03]\n",
            "  [-3.7565341e-03  3.4561057e-03  8.3707999e-03 ... -8.1086652e-03\n",
            "   -9.8724011e-04 -1.7913552e-03]]\n",
            "\n",
            " [[-2.4399965e-03 -7.1386364e-03 -6.4754521e-04 ...  4.2482396e-04\n",
            "   -1.0939958e-03 -2.4961960e-03]\n",
            "  [-8.9483726e-04 -3.4436618e-03 -2.7631135e-03 ... -3.9541279e-04\n",
            "   -2.6790709e-03  5.1913364e-04]\n",
            "  [-2.9887895e-03 -8.0026733e-03 -9.4591314e-03 ... -4.3161246e-03\n",
            "   -4.2537563e-03 -4.3931678e-03]\n",
            "  ...\n",
            "  [ 8.6710947e-03 -3.8991596e-03 -2.4455618e-03 ...  6.1627491e-03\n",
            "   -9.0051731e-03 -4.1408497e-03]\n",
            "  [ 1.3185773e-02 -8.4857401e-03  6.1086845e-04 ...  9.3643041e-04\n",
            "   -1.3897157e-02 -5.8835512e-03]\n",
            "  [ 1.5791850e-02 -4.1207797e-03  1.9230251e-03 ...  1.4613634e-03\n",
            "   -9.7886091e-03 -6.3360371e-03]]\n",
            "\n",
            " [[ 6.0419282e-03 -1.5484646e-03 -1.4837843e-04 ... -7.5947307e-04\n",
            "    3.8284082e-03  5.0869881e-04]\n",
            "  [ 4.8979688e-03  2.9227231e-06 -1.9988527e-03 ... -1.1691718e-03\n",
            "   -6.5643736e-04  2.4445611e-03]\n",
            "  [ 5.1626577e-03 -3.9349440e-03 -1.2889432e-03 ...  5.1594200e-03\n",
            "   -5.6376643e-03 -2.2968501e-03]\n",
            "  ...\n",
            "  [-5.0985492e-03 -2.9919366e-04 -5.3835297e-03 ... -6.7107095e-03\n",
            "   -2.1141658e-03 -1.4683531e-02]\n",
            "  [ 4.0327245e-04 -4.2572934e-03 -2.8183847e-03 ... -5.5249659e-03\n",
            "   -1.3228953e-03 -1.1143382e-02]\n",
            "  [-1.7818622e-04 -1.8184165e-03 -2.5986771e-03 ... -5.7604946e-03\n",
            "   -2.4867638e-03 -6.0461685e-03]]], shape=(64, 20, 65), dtype=float32)\n"
          ]
        }
      ],
      "execution_count": 90,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1bsn5rECU3P",
        "outputId": "c3db1c38-df01-4122-a9d0-0b6ececb7e13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print (len(pred))\n",
        "print(pred)\n",
        "# Notice this is 2d array of length 100 where exach interior array is the prediction for the next character at each time step"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "tf.Tensor(\n",
            "[[-2.9305695e-05 -4.2956294e-03 -5.2133258e-03 ...  9.5791626e-04\n",
            "  -3.4533191e-04  3.2098237e-03]\n",
            " [-1.3752125e-03 -5.8074058e-03 -5.7119508e-03 ...  2.7252191e-03\n",
            "  -1.4251822e-03  1.1254540e-02]\n",
            " [ 5.8382819e-03 -6.6267042e-03 -3.8421443e-03 ...  1.3222675e-03\n",
            "   3.1737301e-03  8.9204768e-03]\n",
            " ...\n",
            " [ 8.2987072e-03  1.0339039e-02  1.2308577e-02 ...  4.0844036e-03\n",
            "   8.1793619e-03  2.5647180e-04]\n",
            " [-1.1709656e-03  1.1991293e-02  4.1806553e-03 ... -2.0262958e-03\n",
            "   1.1298696e-02  2.6900638e-03]\n",
            " [-3.6952903e-03  1.2396922e-02  1.1973148e-02 ...  1.3705932e-03\n",
            "   1.1277175e-02  1.5380629e-03]], shape=(20, 65), dtype=float32)\n"
          ]
        }
      ],
      "execution_count": 91,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTU-0cO6CU3P",
        "outputId": "5f1c8a3a-f636-4a76-9dff-fbc5f8782840"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# and Finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "#and of course its 65 values representing the probanility of each occuring next"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-2.9305695e-05 -4.2956294e-03 -5.2133258e-03 -1.5957048e-03\n",
            " -2.6199180e-03 -6.3154206e-05  2.1698875e-03 -1.5188932e-03\n",
            "  1.5875272e-03  3.0522957e-03  7.5091573e-04  4.7875801e-04\n",
            "  1.6347948e-03  5.2597430e-03 -1.7516541e-03 -1.1778957e-03\n",
            "  2.4833458e-03 -3.6216460e-03 -1.3057920e-03 -7.4356874e-03\n",
            " -4.6790414e-03 -8.2352874e-04 -2.7526062e-04  1.1525935e-04\n",
            "  8.9232239e-04  8.3908549e-04 -1.3037049e-03 -2.3174258e-03\n",
            "  1.3266376e-04 -3.4023642e-03  1.4815219e-03 -5.2332985e-03\n",
            " -3.4534530e-04 -4.2564035e-03 -1.6735424e-03 -1.6155639e-03\n",
            " -2.0279887e-03  2.2832681e-03 -2.3553397e-03 -2.8063357e-04\n",
            " -2.3779168e-03  4.0577451e-04 -1.6055136e-03  9.6513890e-04\n",
            "  1.8542563e-04 -1.0320484e-03  4.3390971e-04  1.9718506e-03\n",
            "  4.0854369e-03 -2.8712134e-04 -1.1179373e-03 -1.1089054e-03\n",
            "  5.7534967e-03 -6.3680205e-03 -3.7487708e-03  3.3414911e-03\n",
            " -1.2157069e-03  5.4902229e-03  2.0733010e-04  3.1038190e-03\n",
            " -8.7722734e-04 -7.6791632e-04  9.5791626e-04 -3.4533191e-04\n",
            "  3.2098237e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ],
      "execution_count": 92,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtmO3UflCU3Q",
        "outputId": "1196f6d0-085c-4c6f-c834-6186c6378e78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If we wantto determine the predicted character we need to sample the output distribution(pick a value based on probabilitis)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual Characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars # and this is what the model predicted for training sequence 1"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'jwAUsXkPUf?K !SHCrvP'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "execution_count": 93,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Hrr8D6nHCU3Q",
        "outputId": "2dc9a8ca-c657-4d19-8bed-260d5a0c114a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Becose we do not have a loss function in tensorflow that can check a 3D array of prediction and tell us oure loss we need to create one of our own  we do it this way\n",
        " so now we need to create a loss function that can compare that output to the expected output and gives us more numerical value repesenting how close the two were"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "beC5xN0VCU3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels,logits, from_logits= True)"
      ],
      "outputs": [],
      "execution_count": 94,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "mnOCkEjVCU3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling the model\n",
        "at this point we can think of our problem as classification problem where the model predicts the probability of each unique letter coming next"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "eAYeSLHmCU3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = loss)"
      ],
      "outputs": [],
      "execution_count": 95,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "MH3NK1AqCU3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Checkpoints\n",
        "now we are going to steup and configure our model to save checkpoints as it trains. this will allow us to load our model and continue training it"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "gBQ4ZSCGCU3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 96,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "UtIbxALRCU3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "SHIow5UeCU3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs = 50, callbacks= [checkpoint_callback])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "829/829 [==============================] - 19s 20ms/step - loss: 2.0672\n",
            "Epoch 2/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.6428\n",
            "Epoch 3/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.5512\n",
            "Epoch 4/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.5016\n",
            "Epoch 5/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.4674\n",
            "Epoch 6/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.4389\n",
            "Epoch 7/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.4120\n",
            "Epoch 8/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.3884\n",
            "Epoch 9/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.3643\n",
            "Epoch 10/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.3432\n",
            "Epoch 11/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.3211\n",
            "Epoch 12/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.3013\n",
            "Epoch 13/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.2824\n",
            "Epoch 14/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.2639\n",
            "Epoch 15/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.2474\n",
            "Epoch 16/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.2314\n",
            "Epoch 17/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.2173\n",
            "Epoch 18/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.2044\n",
            "Epoch 19/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1924\n",
            "Epoch 20/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1812\n",
            "Epoch 21/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1712\n",
            "Epoch 22/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1611\n",
            "Epoch 23/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1531\n",
            "Epoch 24/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1472\n",
            "Epoch 25/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1382\n",
            "Epoch 26/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1337\n",
            "Epoch 27/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1270\n",
            "Epoch 28/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1218\n",
            "Epoch 29/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1173\n",
            "Epoch 30/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1138\n",
            "Epoch 31/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1102\n",
            "Epoch 32/100\n",
            "829/829 [==============================] - 15s 18ms/step - loss: 1.1073\n",
            "Epoch 33/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1039\n",
            "Epoch 34/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1013\n",
            "Epoch 35/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0975\n",
            "Epoch 36/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0965\n",
            "Epoch 37/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0958\n",
            "Epoch 38/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0922\n",
            "Epoch 39/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0921\n",
            "Epoch 40/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0900\n",
            "Epoch 41/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.0898\n",
            "Epoch 42/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0896\n",
            "Epoch 43/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.0880\n",
            "Epoch 44/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.0884\n",
            "Epoch 45/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0869\n",
            "Epoch 46/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0879\n",
            "Epoch 47/100\n",
            "829/829 [==============================] - 15s 18ms/step - loss: 1.0867\n",
            "Epoch 48/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.0884\n",
            "Epoch 49/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.0873\n",
            "Epoch 50/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0872\n",
            "Epoch 51/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0893\n",
            "Epoch 52/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0893\n",
            "Epoch 53/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.0896\n",
            "Epoch 54/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0900\n",
            "Epoch 55/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0916\n",
            "Epoch 56/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0915\n",
            "Epoch 57/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.0937\n",
            "Epoch 58/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0929\n",
            "Epoch 59/100\n",
            "829/829 [==============================] - 15s 18ms/step - loss: 1.0970\n",
            "Epoch 60/100\n",
            "829/829 [==============================] - 15s 18ms/step - loss: 1.0982\n",
            "Epoch 61/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0974\n",
            "Epoch 62/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.0998\n",
            "Epoch 63/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1014\n",
            "Epoch 64/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1018\n",
            "Epoch 65/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1056\n",
            "Epoch 66/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1057\n",
            "Epoch 67/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1083\n",
            "Epoch 68/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1087\n",
            "Epoch 69/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1104\n",
            "Epoch 70/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1135\n",
            "Epoch 71/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1183\n",
            "Epoch 72/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1181\n",
            "Epoch 73/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1222\n",
            "Epoch 74/100\n",
            "829/829 [==============================] - 15s 18ms/step - loss: 1.1229\n",
            "Epoch 75/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1238\n",
            "Epoch 76/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1278\n",
            "Epoch 77/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1300\n",
            "Epoch 78/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1321\n",
            "Epoch 79/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1351\n",
            "Epoch 80/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1397\n",
            "Epoch 81/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1393\n",
            "Epoch 82/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.1439\n",
            "Epoch 83/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1452\n",
            "Epoch 84/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1486\n",
            "Epoch 85/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1520\n",
            "Epoch 86/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1554\n",
            "Epoch 87/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1600\n",
            "Epoch 88/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1625\n",
            "Epoch 89/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1643\n",
            "Epoch 90/100\n",
            "829/829 [==============================] - 15s 18ms/step - loss: 1.1682\n",
            "Epoch 91/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1712\n",
            "Epoch 92/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1770\n",
            "Epoch 93/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1786\n",
            "Epoch 94/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1821\n",
            "Epoch 95/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1871\n",
            "Epoch 96/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1898\n",
            "Epoch 97/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1911\n",
            "Epoch 98/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1977\n",
            "Epoch 99/100\n",
            "829/829 [==============================] - 15s 17ms/step - loss: 1.1999\n",
            "Epoch 100/100\n",
            "829/829 [==============================] - 14s 17ms/step - loss: 1.2041\n"
          ]
        }
      ],
      "execution_count": 97,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrD_64iMCU3k",
        "outputId": "9b002967-edb5-42f0-96e8-eddfb5f68907"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Model\n",
        "we will rebuild the model from a checkpoint using a batch_size of 1  so that we can feed one peice of text to the model and have it make a prediction."
      ],
      "metadata": {
        "id": "D454GqGTG9eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size = 1)"
      ],
      "metadata": {
        "id": "0Q8SAuIfG9G4"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "once the model is finished training we can find the lastest checkpoint that stores the model weights using the following line"
      ],
      "metadata": {
        "id": "rrhDhY9tLud2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "yiX87yoqN11e"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load any checkpoint we want by specifying the exact file to load\n",
        "\n",
        "use this code insteade os above code if you want to use your own checkpoints"
      ],
      "metadata": {
        "id": "51Ira5QpOf5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpoint_num =  10\n",
        "#model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "#model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "TML4yydTOfGk"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating text\n",
        "now we can use this function provided by tensor flow to generate some text using any starting wed'like"
      ],
      "metadata": {
        "id": "n7LP6_7xUZFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step(generating text using the learned model)\n",
        "  #Number of Characters to generate\n",
        "   num_generate = 800\n",
        "\n",
        "   # Converting our start string to numbers(vectorizing)\n",
        "   input_eval = [char2idx[s] for s in start_string]\n",
        "   input_eval = tf.expand_dims(input_eval,0)\n",
        "\n",
        "   # Empty String to store our results\n",
        "   text_generated = []\n",
        "\n",
        "   # Low temperatures results in more predictable text\n",
        "   # Higher temperatures results in more surprising text\n",
        "   # Experiment to find the best setting\n",
        "   Temperature = 1.0\n",
        "\n",
        "   # Here batch size == 1\n",
        "   model.reset_states()\n",
        "   for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # Remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # Using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / Temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1, 0].numpy()\n",
        "\n",
        "    # we pass the predicted Character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "\n",
        "    input_eval = tf. expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "   return (start_string + ''.join(text_generated))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "zJMDjOhsOQsh"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doZZzubh2j7i",
        "outputId": "fdc00fe2-2efd-4599-c9f2-c07336288d4b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: hello\n",
            "hellows.\n",
            "\n",
            "GONZALO:\n",
            "Not in my part.\n",
            "\n",
            "KATHARINA:\n",
            "Love me one.\n",
            "\n",
            "GRUMIO:\n",
            "He wants here you you Val were a princen\n",
            "my honour do thee.\n",
            "\n",
            "GONZALO:\n",
            "What, slave!\n",
            "\n",
            "Provost:\n",
            "Is the loving boar-pent not Let them blose him I take\n",
            "ill smile.\n",
            "\n",
            "LUCIO:\n",
            "I pray thee, met'st me infurning him s: the arment make up myself\n",
            "That thus to go, both your majesty cast out, the\n",
            "horsed cloak--\n",
            "For that, I have heard it but by?\n",
            "\n",
            "BAPTISTA:\n",
            "As ging eyes of golden,\n",
            "Even to the stones abreath\n",
            "As to leave your honour.\n",
            "\n",
            "Citizens:'\n",
            "Be safers that marks the news abruction of me;\n",
            "Lone him: I challed one lies more, or again and sweat\n",
            "And hag seven'd in aide. Katharina, that by this time will command.\n",
            "\n",
            "PRINCE:\n",
            "Come hither.\n",
            "Good Gloucester,\n",
            "Though all the weakill show\n",
            "I live not with him; he cabrings; which s he shall yitwere to Curtons. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONCLUSION\n",
        "Basically you would want to increase the number pf epochs to make the prediction better and also if you have along text of data its better.\n",
        "\n",
        " it is hard to over train this model becouse you would want it to learn the language even more for better predictions\n",
        "\n",
        " you would want the loss to be as little as posible therefore add number of epoch or use a more detailed training data or increase the number of batches by reducing the sequence length"
      ],
      "metadata": {
        "id": "b3xmfbr96jA-"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.9.17",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}